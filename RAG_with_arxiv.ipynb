{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGbu+KPQ3DWNbo21kttomI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noambassat/Rag_on_arxiv/blob/main/RAG_with_arxiv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "c_PrAcFohQ_M"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tI7UPxtGoLbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"https://arxiv.org/list/cs.HC/\" + datetime(2025,3,26).strftime(\"%Y-%m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eJa-MHwEmPQQ",
        "outputId": "c8447a86-96a0-428c-ea73-046c8198a1b4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://arxiv.org/list/cs.HC/2025-03'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGRK7MXDoaqc"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_url(date):\n",
        "  path = \"https://arxiv.org/list/cs.HC/\" + date.strftime(\"%Y-%m\")\n",
        "  content = requests.get(path).text\n",
        "  return [a[\"href\"] for a in bs(content, \"html.parser\").find_all(\"a\",{\"title\":\"View HTML\"})]"
      ],
      "metadata": {
        "id": "rpVc28GhheQ-"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# urls = sum([get_url(datetime(2025, 3, 26)-timedelta(days=i*30))\n",
        "#         for i in trange(5)],[])\n",
        "\n",
        "urls = get_url(datetime(2025, 3, 26))\n",
        "print(urls[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBdKhDhbo8P6",
        "outputId": "fab463b3-8be0-44be-ae39-617a42b3d9b9"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://arxiv.org/html/2503.00144v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article(url):\n",
        "    content = requests.get(url).text\n",
        "    article = bs(content, \"html.parser\").find(\"div\",{\"class\":\"ltx_page_content\"})\n",
        "    return [s.text for s in article.find_all(\"section\")]"
      ],
      "metadata": {
        "id": "7I9zmeBRra0h"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = pd.DataFrame(\n",
        "    {\n",
        "        \"url\":urls,\n",
        "        \"article\":[get_article(url) for url in tqdm(urls)]\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqvUwIOGsuDF",
        "outputId": "535ef526-e6fb-42d2-f74f-5a66d6ea0ef1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/47 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/47 [00:00<00:06,  7.66it/s]\u001b[A\n",
            "  4%|▍         | 2/47 [00:00<00:09,  4.67it/s]\u001b[A\n",
            " 26%|██▌       | 12/47 [06:11<18:02, 30.92s/it]\n",
            "\n",
            "  9%|▊         | 4/47 [00:01<00:17,  2.51it/s]\u001b[A\n",
            " 13%|█▎        | 6/47 [00:01<00:11,  3.62it/s]\u001b[A\n",
            " 15%|█▍        | 7/47 [00:01<00:10,  3.79it/s]\u001b[A\n",
            " 17%|█▋        | 8/47 [00:02<00:09,  4.21it/s]\u001b[A\n",
            " 19%|█▉        | 9/47 [00:02<00:09,  4.07it/s]\u001b[A\n",
            " 21%|██▏       | 10/47 [00:02<00:11,  3.30it/s]\u001b[A\n",
            " 23%|██▎       | 11/47 [00:03<00:10,  3.40it/s]\u001b[A\n",
            " 26%|██▌       | 12/47 [00:03<00:08,  4.00it/s]\u001b[A\n",
            " 28%|██▊       | 13/47 [00:03<00:08,  4.14it/s]\u001b[A\n",
            " 30%|██▉       | 14/47 [00:04<00:15,  2.18it/s]\u001b[A\n",
            " 32%|███▏      | 15/47 [00:04<00:11,  2.83it/s]\u001b[A\n",
            " 34%|███▍      | 16/47 [00:04<00:10,  2.84it/s]\u001b[A\n",
            " 36%|███▌      | 17/47 [00:05<00:09,  3.26it/s]\u001b[A\n",
            " 38%|███▊      | 18/47 [00:05<00:11,  2.61it/s]\u001b[A\n",
            " 40%|████      | 19/47 [00:05<00:09,  3.02it/s]\u001b[A\n",
            " 45%|████▍     | 21/47 [00:06<00:06,  4.10it/s]\u001b[A\n",
            " 47%|████▋     | 22/47 [00:06<00:05,  4.71it/s]\u001b[A\n",
            " 49%|████▉     | 23/47 [00:07<00:08,  2.72it/s]\u001b[A\n",
            " 51%|█████     | 24/47 [00:07<00:07,  3.00it/s]\u001b[A\n",
            " 53%|█████▎    | 25/47 [00:07<00:07,  3.07it/s]\u001b[A\n",
            " 55%|█████▌    | 26/47 [00:07<00:05,  3.69it/s]\u001b[A\n",
            " 57%|█████▋    | 27/47 [00:07<00:04,  4.03it/s]\u001b[A\n",
            " 60%|█████▉    | 28/47 [00:08<00:05,  3.78it/s]\u001b[A\n",
            " 62%|██████▏   | 29/47 [00:08<00:06,  2.62it/s]\u001b[A\n",
            " 64%|██████▍   | 30/47 [00:09<00:06,  2.69it/s]\u001b[A\n",
            " 66%|██████▌   | 31/47 [00:09<00:05,  2.90it/s]\u001b[A\n",
            " 68%|██████▊   | 32/47 [00:09<00:04,  3.20it/s]\u001b[A\n",
            " 70%|███████   | 33/47 [00:10<00:05,  2.71it/s]\u001b[A\n",
            " 72%|███████▏  | 34/47 [00:10<00:04,  2.96it/s]\u001b[A\n",
            " 74%|███████▍  | 35/47 [00:10<00:03,  3.46it/s]\u001b[A\n",
            " 77%|███████▋  | 36/47 [00:10<00:02,  3.88it/s]\u001b[A\n",
            " 79%|███████▊  | 37/47 [00:11<00:02,  4.02it/s]\u001b[A\n",
            " 81%|████████  | 38/47 [00:11<00:03,  2.41it/s]\u001b[A\n",
            " 83%|████████▎ | 39/47 [00:12<00:03,  2.53it/s]\u001b[A\n",
            " 85%|████████▌ | 40/47 [00:12<00:02,  2.72it/s]\u001b[A\n",
            " 87%|████████▋ | 41/47 [00:12<00:02,  2.83it/s]\u001b[A\n",
            " 89%|████████▉ | 42/47 [00:13<00:02,  2.11it/s]\u001b[A\n",
            " 91%|█████████▏| 43/47 [00:13<00:01,  2.52it/s]\u001b[A\n",
            " 94%|█████████▎| 44/47 [00:14<00:00,  3.17it/s]\u001b[A\n",
            " 96%|█████████▌| 45/47 [00:14<00:00,  3.52it/s]\u001b[A\n",
            " 98%|█████████▊| 46/47 [00:14<00:00,  3.80it/s]\u001b[A\n",
            "100%|██████████| 47/47 [00:15<00:00,  3.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles[\"article\"].isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4wuys1ut9k3",
        "outputId": "93aaccc2-e8df-4e8a-ea0c-c978ff2db58e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Vro78dwp3z",
        "outputId": "2009f698-ca59-4b29-d751-567fdb0b54f8"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "42K6Zjzrvj62",
        "outputId": "9259a5df-6370-419f-8412-c10094b0bbaf"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   url  \\\n",
              "0  https://arxiv.org/html/2503.00144v1   \n",
              "1  https://arxiv.org/html/2503.00149v2   \n",
              "2  https://arxiv.org/html/2503.00228v1   \n",
              "3  https://arxiv.org/html/2503.00257v1   \n",
              "4  https://arxiv.org/html/2503.00303v1   \n",
              "\n",
              "                                             article  \n",
              "0  [\\n\\n1 Introduction\\n\\nProgramming is an impor...  \n",
              "1  [\\n\\n1. Introduction\\n\\nTactile charts are an ...  \n",
              "2  [\\n\\n1. Introduction\\n\\nSimilarity is a fundam...  \n",
              "3  [\\n\\n1. Introduction\\n\\nThe Japanese manga111M...  \n",
              "4  [\\n\\nI Introduction\\n\\n\\nThis document is a mo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb995fe2-0386-404f-8681-019b5229c957\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://arxiv.org/html/2503.00144v1</td>\n",
              "      <td>[\\n\\n1 Introduction\\n\\nProgramming is an impor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://arxiv.org/html/2503.00149v2</td>\n",
              "      <td>[\\n\\n1. Introduction\\n\\nTactile charts are an ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://arxiv.org/html/2503.00228v1</td>\n",
              "      <td>[\\n\\n1. Introduction\\n\\nSimilarity is a fundam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://arxiv.org/html/2503.00257v1</td>\n",
              "      <td>[\\n\\n1. Introduction\\n\\nThe Japanese manga111M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://arxiv.org/html/2503.00303v1</td>\n",
              "      <td>[\\n\\nI Introduction\\n\\n\\nThis document is a mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb995fe2-0386-404f-8681-019b5229c957')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb995fe2-0386-404f-8681-019b5229c957 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb995fe2-0386-404f-8681-019b5229c957');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-972042ed-673e-40e8-ac52-4cfaa4694f9c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-972042ed-673e-40e8-ac52-4cfaa4694f9c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-972042ed-673e-40e8-ac52-4cfaa4694f9c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "articles",
              "summary": "{\n  \"name\": \"articles\",\n  \"rows\": 47,\n  \"fields\": [\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"https://arxiv.org/html/2503.02007v1\",\n          \"https://arxiv.org/html/2503.02637v1\",\n          \"https://arxiv.org/html/2503.01769v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = articles.explode('article').rename(columns={\"article\":\"paragraph\"}) # for each url there are several paragraph\n",
        "paragraphs = paragraphs[paragraphs['paragraph'].str.split().map(len)>10]\n",
        "paragraphs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HDMChC9mu3i6",
        "outputId": "a12c9ec7-4a9d-471a-e526-32a9a81bcf70"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   url  \\\n",
              "0  https://arxiv.org/html/2503.00144v1   \n",
              "0  https://arxiv.org/html/2503.00144v1   \n",
              "0  https://arxiv.org/html/2503.00144v1   \n",
              "0  https://arxiv.org/html/2503.00144v1   \n",
              "0  https://arxiv.org/html/2503.00144v1   \n",
              "\n",
              "                                           paragraph  \n",
              "0  \\n\\n1 Introduction\\n\\nProgramming is an import...  \n",
              "0  \\n\\n2 Related Work\\n\\n\\n2.0.1 Learner-System C...  \n",
              "0  \\n\\n2.0.1 Learner-System Control Dynamics.\\n\\n...  \n",
              "0  \\n\\n2.0.2 Participatory Design in Educational ...  \n",
              "0  \\n\\n3 Participatory Design with Learners and I...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68e365a8-109d-4e5d-9927-d8c271202636\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>paragraph</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://arxiv.org/html/2503.00144v1</td>\n",
              "      <td>\\n\\n1 Introduction\\n\\nProgramming is an import...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://arxiv.org/html/2503.00144v1</td>\n",
              "      <td>\\n\\n2 Related Work\\n\\n\\n2.0.1 Learner-System C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://arxiv.org/html/2503.00144v1</td>\n",
              "      <td>\\n\\n2.0.1 Learner-System Control Dynamics.\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://arxiv.org/html/2503.00144v1</td>\n",
              "      <td>\\n\\n2.0.2 Participatory Design in Educational ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://arxiv.org/html/2503.00144v1</td>\n",
              "      <td>\\n\\n3 Participatory Design with Learners and I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68e365a8-109d-4e5d-9927-d8c271202636')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68e365a8-109d-4e5d-9927-d8c271202636 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68e365a8-109d-4e5d-9927-d8c271202636');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8fbcb6c7-2697-4f5b-9ec5-69a1383f1e31\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8fbcb6c7-2697-4f5b-9ec5-69a1383f1e31')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8fbcb6c7-2697-4f5b-9ec5-69a1383f1e31 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "paragraphs",
              "summary": "{\n  \"name\": \"paragraphs\",\n  \"rows\": 1695,\n  \"fields\": [\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"https://arxiv.org/html/2503.02007v1\",\n          \"https://arxiv.org/html/2503.02637v1\",\n          \"https://arxiv.org/html/2503.01769v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paragraph\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1695,\n        \"samples\": [\n          \"\\n\\n2.3. Findings\\n\\n\\n2.3.1. Personalization Challenges during Communication to Diverse Patients\\n\\nOur participants identified several challenges during the interviews, all converging on the critical issue of personalization in communication. Students described feeling unprepared for some patient interactions due to the patient\\u2019s condition, such as pain or visible injuries, emotional state, and diverse backgrounds. For example, one student described difficulty in the early days of their internship because they were not used to seeing patients in severe conditions hooked up to complex machines, as one DPT student said: \\u201c\\u2026I think I learned, I got very acclimated to it, and I very much enjoyed it, but when I walked in, [\\u2026] it\\u2019s my first day, [\\u2026] and I saw somebody with end-stage renal disease, and I was there on my first day, that was not something I was prepared for at all\\u2026(P5)\\u201d Some other students described challenges interacting with patients who were aggressive or refused to engage with them: \\u201c\\u2026Also, in certain settings, the patient could just be like, really overwhelmed, and have like difficulty answering some questions, or there may be like really sensitive topics, that they don\\u2019t feel comfortable answering, so it can be hard to get information from them in those cases\\u2026(P1)\\u201d Our participants also described the diverse backgrounds that patients come from as posing challenges, including treating patients who do not speak English, necessitating an interpreter, or those with low levels of medical literacy, for whom they would need to adapt how they explain conditions and treatments. Overall, students described how patients\\u2019 conditions, dispositions, and backgrounds varied widely in the contexts they worked in, requiring a great deal of adaptability. Personalizing their communication for all their patient needs was a challenge for them, particularly at the beginning of their clinical work.\\n\\n\\n\\n\\n2.3.2. Mismatch between Training and Clinical Communication\\n\\nWhen asked about the classroom-based training they received for clinical communication, students pointed to limited opportunities to practice authentic communication, and described a mismatch between their training and what they needed to do in practice. Many students described learning on the job and through their clinical experience as the best way to learn the interpersonal communication skills they needed for their profession. While they did receive training on communication in their coursework, they often identified mismatches between that training and what they needed to do in clinical settings. For example, students described simulation lab activities where they interacted with a standardized patient as beneficial practices, but noted it was insufficient because they only used this lab once or twice a year. Furthermore, they described how their practice opportunities were not always realistic in terms of the interactions they would have with patients. For instance, one DPT student described needing to communicate with people unfamiliar with medical terms versus classroom activities where they practiced with their peers who have advanced medical knowledge, as she stated: \\u201c\\u2026So [in the lab practical], we\\u2019re like communicating with our patients. But our patients are classmates. So I feel like just being put in that situation where you have to talk to like an actual patient for that case\\u2026(P6)\\u201d. Students also frequently described patients who did not speak English but did not receive training in communicating through interpreters. As one PA student noted: \\u201c\\u2026or the patient doesn\\u2019t really understand what the interpreter saying, or we can\\u2019t get the exact language that the patient speaks (P1)\\u2026\\u201d.\\n\\n\\nThey also described the ways traditional classroom activities were not as helpful due to the evaluation systems. Students described being focused on getting a good grade and following checklists prescribed by their instructors that added pressure to their simulation and role-play activities in classes and interfered with their ability to practice authentically and flexibly, as one PharmD student said:\\u201c\\u2026When we do the simulation labs and just like sit in the corner and like\\u2026 you know that they\\u2019re (faculty) looking. You know that they\\u2019re listening to you, and you\\u2019re trying to just like drown them out, which can make you feel more nervous because you start to think like I need to make sure I\\u2019m saying everything to make sure I\\u2019m hitting all the points (P4)\\u2026\\u201d Grading pressure rendered the training environment less reflective of real clinical settings and, in their view, less meaningful or practical. For example, one PharmD student shared: \\u201c\\u2026I think just working with real people, you have a little bit more of a curveball with it. And you just like\\u2026 everything on that checklist that I used in school, I don\\u2019t really use that, because it\\u2019s just different scenarios that are going on\\u2026(P3)\\u201d\\n\\n\\n\\n\\n2.3.3. VR as an Opportunity for More Realistic and Varied Practice Opportunities\\n\\nDuring our interviews, most students expressed positive perceptions regarding increased practice opportunities. Notably, participants highlighted the potential of VR as an additional tool for diverse and varied practice scenarios, particularly for extreme situations that are challenging to replicate in standard simulations but are often encountered in real practice. For example, one PA student remarked: \\u201c\\u2026It could really be used for like so many different things [\\u2026] like, what do I do if a patient starts crying in the middle of a conversation, or how do I bring up bad news or things like that?..(P1)\\u201d. Furthermore, we identified their expectations for various interactive features in VR, including unpredictable conversational dialogues, patient data visualizations, opportunities for self-reflection, and multi-user communication and collaboration. Overall, students identified realistic scenarios and cases to recreate real clinical situations as the most essential factor in VR communication training.\\n\\n\\n\",\n          \"\\n\\n6. Conclusion\\n\\nReward function design plays a crucial role for RL-based biomechanical simulations. Using a choice reaction task as a test-bed, we have analysed the individual and combined effects of three essential reward function components, namely task completion, target proximity, and effort terms. Our simulation study reveals that a combination of sparse completion bonus and dense proximity rewards is essential for task success.\\nInterestingly, effort terms are dispensable if appropriate proximity rewards are used; otherwise, they need to be carefully weighted.\\nOur work emphasises the need for a better understanding of the subtleties involved in training musculoskeletal models, for a variety of interaction tasks. By providing guidelines and first principles for reward function design, this work contributes towards the use of RL-based user simulations as a practical tool for HCI research and design.\\n\\n\\nAcknowledgements.\\nThis work was supported by EPSRC grant EP/W02456X/1.\\nHannah Selder and Arthur Fleig acknowledge the financial support by the Federal Ministry of Education and Research of Germany and by S\\u00e4chsische Staatsministerium f\\u00fcr Wissenschaft, Kultur und Tourismus in the programme Center of Excellence for AI-research \\u201eCenter for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzig\\u201c, project identification number: ScaDS.AI.\\n\\n\\n\",\n          \"\\n\\n4. Findings\\n\\n\\n4.1. What Roles Did Participants Expect from VLM Camera Sensor-based DIY Smart Homes?\\n\\nThis section explains the roles that participants envisioned for DIY smart home features using VLM camera sensors, incorporating the three camera configuration concepts. Participants developed features in three primary roles: auto-monitoring (Section 4.1.1), assistant (Section 4.1.2), and advisory (Section 4.1.3). For a detailed list of the features created by participants, please refer to Appendix (Table 5).\\n\\n\\n\\n4.1.1. Auto-Monitoring Role\\n\\nTable 2. Examples of participant ideas for auto-monitoring role features\\n\\n\\n\\nDeveloped Feature\\nExample Situation\\nIf this\\nThen that\\n\\n\\n\\n\\n\\n\\n\\nAlerts when electricity\\n\\n\\nis being wasted\\n\\n\\nunnecessarily (P8)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf unnecessary\\n\\n\\nappliances\\n\\n\\nare running\\n\\n\\n\\n\\n\\n\\nTurn off\\n\\n\\nunnecessary\\n\\n\\nappliances\\n\\n\\n\\n\\n\\n\\n\\n\\nAlerts for\\n\\n\\ndangerous situations\\n\\n\\nin the kitchen (P1)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf a potentially dangerous\\n\\n\\nsituation occurs\\n\\n\\nwhile cooking\\n\\n\\n\\n\\n\\n\\nAlert about\\n\\n\\nthe dangerous\\n\\n\\nsituation\\n\\n\\n\\n\\n\\n\\n\\n\\nAutomatic pet care\\n\\n\\nfor when pets\\n\\n\\nneed help (P11)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf the dog shows\\n\\n\\nseparation anxiety\\n\\n\\nwhile alone at home\\n\\n\\n\\n\\n\\n\\nPlay white noise\\n\\n\\nto reduce anxiety\\n\\n\\n\\n\\n\\n\\n\\n\\nAlerts when\\n\\n\\nposture needs\\n\\n\\ncorrection (P4)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf I am working\\n\\n\\nand need\\n\\n\\nposture correction\\n\\n\\n\\n\\n\\n\\nPlay a voice prompt\\n\\n\\nto remind me\\n\\n\\nto correct my posture\\n\\n\\n\\n\\n\\n\\n\\n\\nTurns on the light\\n\\n\\nwhen the child is\\n\\n\\ndoing activities without\\n\\n\\nsufficient lighting (P2)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf the child is doing\\n\\n\\nan activity at the desk\\n\\n\\nwithout sufficient lighting\\n\\n\\n\\n\\n\\n\\nAdjust lighting\\n\\n\\nand play\\n\\n\\nappropriate music\\n\\n\\n\\n\\n\\n\\n\\n\\nParticipants leveraged the wide field of view and situational analysis capabilities of VLM camera sensors to develop DIY smart home features for automatic home monitoring. These features tracked the home environment, user behavior, and family members\\u2019 situations, triggering alerts or automated actions when necessary (Table 2). The goal was to ensure automatic monitoring of critical home situations requiring attention.\\n\\n\\nParticipants built features that leveraged the camera sensor for auto-monitoring homes, allowing them to receive notifications about spaces that required management in terms of hygiene, tidiness, safety, and energy efficiency.\\n\\n\\n\\nEven if I\\u2019m not looking, if there\\u2019s an unsafe situation in the kitchen, like an egg about to fall, a knife slipping, or smoke starting, I thought of a feature where the camera could alert me. (P1)\\n\\n\\n\\nAdditionally, participants developed features to enhance awareness of their own and their family members\\u2019 situations. By utilizing camera sensors to track their behavior, they identified unconscious habitual actions, fostering opportunities to adopt healthier lifestyle habits. These auto-monitoring features were especially useful for households with children or pets requiring constant care. Participants created features to detect moments when children or pets might need assistance, even in their absence, enabling the system to send notifications or trigger automatic responses. This approach supported more efficient caregiving tasks and promoted a safer home environment.\\n\\nI\\u2019ve been trying to stop biting my nails for almost 10 years, but I still haven\\u2019t kicked the habit. Since it\\u2019s something I do subconsciously when I\\u2019m focused, I thought it would be great if the camera could detect my behavior and warn me. (P10)\\n\\n\\nI built a smart home feature that triggers commands like \\u2018No,\\u2019 \\u2018Stop,\\u2019 or \\u2018Wait\\u2019 in my voice when my dog chews on the couch or pees in the wrong place, so the situation can be addressed immediately. (P11)\\n\\n\\n\\nParticipants anticipated that auto-monitoring features would provide reassurance that no concerning situations would occur at home, even when they were away or occupied. They also viewed the ability to analyze daily life through the camera sensor as an opportunity to gain an objective perspective on their home. This perspective enabled them to recognize overlooked issues and reflect on familiar routines, making it a satisfying experience for them.\\n\\nIf the smart home tells me my desk is messy, I can objectively see that, yes, it is messy. I think the camera could also bring attention to situations I subconsciously know but am avoiding. [\\u2026] It gave me great satisfaction to have the opportunity to step back and reflect on the familiar scenery of my home from a fresh perspective. (P10)\\n\\n\\n\\nIn conclusion, participants developed features to automatically monitor home situations using VLM camera sensors, enabling alerts or automatic actions during critical moments that required attention to the state of the home, family members, or their own behavior.\\n\\n\\n\\n\\n4.1.2. Assistant Role\\n\\nTable 3. Examples of participant ideas for assistant role features\\n\\n\\n\\nDeveloped Feature\\nExample Situation\\nIf this\\nThen that\\n\\n\\n\\n\\n\\n\\n\\nGuides organization\\n\\n\\nwhen items are not\\n\\n\\narranged according\\n\\n\\nto the rules (P7)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf drinks of the\\n\\n\\nsame type are placed\\n\\n\\nin different sections\\n\\n\\n\\n\\n\\n\\nPrompt me\\n\\n\\nto sort and\\n\\n\\norganize the items\\n\\n\\n\\n\\n\\n\\n\\n\\nSequentially guides\\n\\n\\nme through\\n\\n\\na coffee recipe (P7)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf I start brewing coffee\\n\\n\\nusing a hand drip,\\n\\n\\nprompt me to choose\\n\\n\\nwhich recipe to follow\\n\\n\\n\\n\\n\\n\\nProvide step-by-step\\n\\n\\nguidance on\\n\\n\\nwhat to do next\\n\\n\\n\\n\\n\\n\\n\\n\\nHelps meticulously\\n\\n\\ncheck the condition\\n\\n\\nof clothes (P9)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf I\\u2019m organizing clothes\\n\\n\\nin front of the closet\\n\\n\\n\\n\\n\\n\\nRecommend clothes\\n\\n\\nfor the AirDresser\\n\\n\\nand activate\\n\\n\\nthe appropriate mode\\n\\n\\n\\n\\n\\n\\n\\n\\nAssists with\\n\\n\\nstudying (P8)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf I open\\n\\n\\na book to study\\n\\n\\n\\n\\n\\n\\nAnalyze the key points\\n\\n\\nof the page and\\n\\n\\nsummarize the content\\n\\n\\n\\n\\n\\n\\n\\n\\nAssists with baby\\n\\n\\ncare actions (P8)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhen my nephew visits,\\n\\n\\nanalyze the\\n\\n\\nbaby\\u2019s expression\\n\\n\\n\\n\\n\\n\\nAnalyze whether\\n\\n\\nappropriate caregiving\\n\\n\\nactions were taken\\n\\n\\n\\n\\n\\n\\n\\n\\nParticipants perceived VLM sensors as more meticulous and sensitive than themselves, with the ability to provide useful information about current situations. Leveraging these strengths, participants developed features that offered enriched support for their daily routines (Table 3).\\n\\n\\nParticipants envisioned the camera sensor as an assistant that analyzes situations alongside them, especially in cases where human judgment might miss subtle details. This role was particularly useful for detecting subtle changes or reviewing large volumes of information. For those with limited knowledge of certain situations, the assistant feature supported decision-making, which was especially valuable for participants caring for infants or pets. For example, P8, who cared for their niece, and P7, who looked after their partner\\u2019s pet, developed features to monitor non-verbal behaviors, enabling them to assess the comfort of those under their care.\\n\\nCameras can detect subtle changes like dry soil or drooping leaves better than a person. (P5)\\n\\n\\nThe camera can recognize subtle changes in the baby\\u2019s expression when I take a caregiving action. If the expression turns positive, it means I did the right thing, so I think it would be very effective in that sense. (P8)\\n\\n\\n\\nAdditionally, features that provided useful information about the current situation played a role in helping participants better understand and become more aware of their own circumstances.\\n\\nI\\u2019ve developed a habit of checking how much sugar is in food. I thought it would be really convenient if the camera could do that for me, so I built this feature. (P8)\\n\\n\\n\\nParticipants expressed satisfaction with the assistant features, as they enabled the effortless acquisition of useful information in daily life. This information supported the maintenance of an ideal routine and highlighted important but previously overlooked aspects of their everyday lives.\\n\\n\\nIn conclusion, participants developed assistant features that supported their activities and enriched their daily lives. These features offered new information and perspectives, making daily life more meaningful and fulfilling.\\n\\n\\n\\n\\n4.1.3. Advisory Role\\n\\nTable 4. Examples of participant ideas for advisory role features\\n\\n\\n\\nDeveloped Feature\\nExample Situation\\nIf this\\nThen that\\n\\n\\n\\n\\n\\n\\n\\nChecks the condition\\n\\n\\nof plants and\\n\\n\\nsuggests solutions (P5)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1) If a change in\\n\\n\\nleaf color or damage\\n\\n\\nis detected\\n\\n\\n2) If insects are found\\n\\n\\non a specific plant\\n\\n\\n\\n\\n\\n\\nProvide guidance\\n\\n\\non how to address\\n\\n\\nthe issue\\n\\n\\n\\n\\n\\n\\n\\n\\nOffers advice for\\n\\n\\nefficient tidying (P10)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf the desk is\\n\\n\\ncluttered\\n\\n\\n\\n\\n\\n\\nSuggest how to\\n\\n\\norganize items\\n\\n\\nto tidy the desk\\n\\n\\n\\n\\n\\n\\n\\n\\nHelps manage the\\n\\n\\nfamily\\u2019s nutrition (P1)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAfter analyzing\\n\\n\\nthe food and\\n\\n\\nsupplements consumed\\n\\n\\nby family members\\n\\n\\n\\n\\n\\n\\nAdvise on\\n\\n\\nwhich nutrients are\\n\\n\\nlacking and suggest\\n\\n\\nappropriate supplements\\n\\n\\n\\n\\n\\n\\n\\n\\nRecommends outfits\\n\\n\\nbased on the weather\\n\\n\\nand harmony (P7)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf I\\u2019m selecting clothes\\n\\n\\nn the dressing room\\n\\n\\n\\n\\n\\n\\nRecommend the\\n\\n\\nremaining outfit pieces\\n\\n\\nbased on the weather\\n\\n\\nand overall harmony\\n\\n\\n\\n\\n\\n\\n\\n\\nAnalyzes the\\n\\n\\ncooking process\\n\\n\\nand provides tips (P4)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf inefficient or unhygienic\\n\\n\\nmoments are detected\\n\\n\\nduring cooking\\n\\n\\n\\n\\n\\n\\nProvide advice on\\n\\n\\nimproving the\\n\\n\\ncooking process\\n\\n\\n\\n\\n\\n\\n\\n\\nAnalyzes sleeping posture\\n\\n\\nand offers\\n\\n\\nsuitable advice (P12)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIf poor sleeping\\n\\n\\npostures are\\n\\n\\ndetected during sleep\\n\\n\\n\\n\\n\\n\\nUpon waking,\\n\\n\\nsuggest stretches or\\n\\n\\nfoods that benefit the\\n\\n\\naffected body part\\n\\n\\n\\n\\n\\n\\n\\n\\nParticipants aimed to go beyond simple convenience by developing features that analyze situations, identify root causes, and propose solutions. These features were intended to detect previously unrecognized problems and provide immediate support for their resolution (Table 4).\\n\\n\\nThey leveraged camera sensors\\u2019 analytical capabilities to identify unnoticed issues and explore ways to improve their daily lives. For example, P4 developed a feature to analyze their cooking routine, identifying inefficient movements and potential hygiene risks. Similarly, participants created features to analyze specific problem situations, aiming to identify their root causes and facilitate effective solutions.\\n\\nI\\u2019ve had a few instances where I woke up feeling stiff, so I thought it would be helpful to analyze whether I\\u2019m adopting any bad sleeping postures during the night. (P12)\\n\\n\\n\\nInstead of merely diagnosing situations, participants leveraged the VLM-based camera sensor to develop smart home features that identified problem causes and proposed solutions. For example, P9 developed a feature that suggests tidying up distracting items before starting a study session, while P10 created a feature that provides personalized advice for more effective tidying and organization.\\n\\n\\nThey expressed satisfaction with the advisory features, appreciating their ability to deliver tailored, professional-like guidance specific to their home environment. A key advantage of these features was their capacity to provide personalized, actionable solutions that accounted for the current state of household items, available ingredients, furniture layout, and appliances.\\n\\nI\\u2019m not good at organizing, so even though I have enough storage space, I often don\\u2019t know where to put things. [\\u2026] The information from YouTube or blogs doesn\\u2019t help much because their spaces are structured differently from mine. However, the feature that my home could be analyzed to give me customized storage advice was the best part. (P10)\\n\\n\\nI wanted to create features that didn\\u2019t just turn lights on and off but could offer fundamental advice, like telling me what was wrong and how I could fix it to make things better. (P4)\\n\\n\\n\\nIn conclusion, participants aimed to create smart home features that function as advisors, helping them identify opportunities to improve daily habits and receive personalized advice to enhance their quality of life.\\n\\n\\n\\n\\n\\n4.2. How did the characteristics of VLM camera sensors impact the DIY process?\\n\\nThis section outlines the key characteristics of VLM-based camera sensors and explains the new potential they introduced in the DIY setup process, along with the important factors that need to be considered.\\n\\n\\n\\n4.2.1. Comprehensive Sensing: Building Features That Adapt Flexibly to Various Situations\\n\\nVLM camera sensors showed the capability to understand the current context without user intervention. Building on this capability, participants constructed more comprehensive rules. The potential of these comprehensive rules led participants to focus less on the specifics of each feature and instead increasingly consider the \\u201cfundamental goals of the smart home.\\u201d\\n\\nI didn\\u2019t have to build separate features for each situation, like when my dog howled or chewed on the sofa. The ability to cover many situations under a single feature, such as \\u201cDog Care Alert,\\u201d was a major advantage. (P11)\\n\\n\\n\\nThis approach enabled a flexible smart home feature that reduced the need for participants to specify every scenario while allowing the system to effectively respond to situations the user may have overlooked.\\n\\nI had to specify each risky situation before, but now, using a command like \\u2018alert me for any dangerous situation\\u2019 covers risks I hadn\\u2019t even considered. (P1)\\n\\n\\n\\nHowever, developing smart home features using comprehensive rules presented a challenge. When creating comprehensive features covering multiple situations, it was critical to carefully adjust the rule\\u2019s level to ensure it was broad enough to address diverse situations while accurately reflecting user intent.\\n\\nWhen I asked it to analyze my style, I expected it to include my hairstyle, but it only analyzed my fashion. So, I had to adjust it to include both. (P8)\\n\\n\\n\\nIn conclusion, VLM camera sensors offer the potential to build smart home features that adapt flexibly to various situations. However, successful implementation required careful consideration of how to set the command level.\\n\\n\\n\\n\\n4.2.2. Indirect Sensing Through Inference: Using Unobserved Factors for Feature Execution\\n\\nThrough inference capabilities, VLM camera sensors held the potential to infer elements that were not directly captured on camera, further expanding their sensing role. Leveraging this capability, participants used elements not captured on the image as sources for executing features.\\n\\nDust and hair are hard to see, and monitoring the entire floor isn\\u2019t realistic either, so instead, I set it up to infer where the floor is dirty based on human activities. For example, if someone dries their hair, it suggests cleaning that area. (P3) - Figure 12\\n\\n\\n\\nFigure 12. Photos of P3 used to infer a dirty space based on human activities\\n\\n\\nThe ability to infer unseen elements meant that participants did not need to capture every important detail on camera to use smart home features. This potential also provided significant advantages in terms of efficiency and privacy.\\n\\nTo be honest, it feels burdensome when my facial expressions are being recorded while I\\u2019m working. Instead, I thought having the camera analyze my behaviors to understand my emotions would be a better way. (P3) - Figure 13-(a)\\n\\n\\nFor the feature that estimates my calorie expenditure while squatting, using a first-person perspective is more practical than filming from a third-person view. It protects my privacy by not capturing my appearance and allows me to work out anywhere with just one sensor. (P9) - Figure 13-(b)\\n\\n\\n\\n\\n\\n\\n(a) \\n\\n\\n\\n\\n(b) \\n\\n\\n\\nFigure 13. Participants\\u2019 photos, (a) P3 developed a feature to infer emotions such as focus, nervousness, or anger based on hand movements like opening a book, playing with a rubber band, or tightly clenching her fist, (b) P9 developed a feature to count squats and calculate calories burned by tracking changes in her field of view\\n\\n\\nHowever, due to the sensor\\u2019s inference capabilities, participants experienced confusion about what the sensor could and could not detect. At times, the sensor even provided fabricated or false information, which led to a decline in participants\\u2019 trust.\\n\\nOne time, the system said there was food waste in the sink when the sink wasn\\u2019t even visible in my images. That made me question whether all the meaningful inference and helpful guidance it had given me before was a lie. I felt really betrayed. (P4)\\n\\n\\n\\nIn conclusion, the VLM camera sensor demonstrated the potential to expand its role by inferring unseen elements. While this capability offered significant benefits in terms of efficiency and privacy, it also blurred the lines of what the sensor could reliably sense, making it harder for participants to differentiate between its capabilities and limitations.\\n\\n\\n\\n\\n4.2.3. Perspective-embodied Sensing: Enabling Situation Analysis from Diverse Perspectives\\n\\nParticipants were able to utilize various types of camera sensors when building a single feature, enabling them to sense a situation from user-focused, device-focused, and spatial-focused perspectives depending on the configuration of the camera sensor they selected. The VLM camera sensor, taking into account these diverse perspectives, was able to analyze situations in a way that reflected the chosen perspective. In the user-focused perspective (e.g., wearable cameras), the sensor recognized that the scene captured on the screen represented the participant\\u2019s current view, allowing it to focus on the objects or elements that the participant was observing or interacting with. In the device-focused perspective (e.g., cameras embedded in devices), the context of the device\\u2019s purpose and function was considered in the analysis. For example, from a refrigerator\\u2019s perspective, the camera\\u2019s role could be focused on monitoring food freshness, tracking inventory, and supporting automated grocery shopping or meal planning. For the spatial-focused perspective (e.g., 360-degree cameras), the analysis went beyond the user-centered approach seen in wearable cameras, taking into account the broader contextual surroundings, such as the overall atmosphere of the home. This diversity in perspective enabled the sensor to interpret the home situation in a more contextually rich manner.\\n\\nSince the camera attached to the mirror doesn\\u2019t capture the mirror itself, it just looks like a person is standing still in the image. So, I had to specify that this image was taken by a mirror-embedded camera, which helped the sensor better understand that the person standing in the center was actually checking their outfit in the mirror. (P8)\\n\\n\\n\\nFurthermore, the ability to explicitly define the camera sensor\\u2019s perspective allowed participants to assign both physical perspectives (user, device, or spatial) and conceptual perspectives, such as those of a mother, an expert, or a pet.\\n\\nWe only see as much as we know. Since I\\u2019m not a plant expert, I thought that analyzing it using a plant expert\\u2019s analytical approach could help identify elements I might have missed and support the diagnosis of the plant\\u2019s condition. (P5)\\n\\n\\n\\nOn the other hand, relying on a single camera perspective was sometimes insufficient to capture all critical information. To address this, participants recognized the need for multi-perspective sensing and envisioned collaboration between multiple cameras. However, developing such a system required clear rules and conditions to determine when and how camera perspectives should switch. Participants expressed the need for support in defining these rules to ensure smooth and effective camera transitions.\\n\\nTo understand what\\u2019s happening with the baby, the camera needs a wide-angle view, but if the baby\\u2019s facial expressions are important, it\\u2019ll need a close-up. It\\u2019s a dilemma. So, I thought about using both cameras together\\u2014a wide-angle camera to capture the overall situation and a wearable camera to focus on the details. [\\u2026] But in that case, I would need to set rules for when to switch between the cameras, and that could make things a bit more complicated. (P8)\\n\\n\\n\\nIn conclusion, VLM camera sensors enabled participants to analyze a single situation from multiple perspectives. Notably, when a single feature required the collaboration of cameras with different perspectives, participants emphasized the need for support in defining rules and conditions for seamless camera switching.\\n\\n\\n\\n\\n4.2.4. Unbounded Sensor Values: Enjoying Unexpected Features\\n\\nUnlike commonly used IoT sensors, which produce results within a limited range of values, VLM sensors offer the possibility of generating an unlimited range of outputs. Since everyday situations can vary greatly, the results generated by the camera sensor extend far beyond a fixed scope. Initially, participants tried to control this infinite potential by restricting the range of sensor values to predefined lists. However, this approach often ended up overly limiting the sensor\\u2019s capabilities.\\n\\nI tried limiting the responses to just two options: either the child is trying to climb the stairs or not. [\\u2026] When it came to analyzing what the kids were doing in the living room, I wanted to leave the responses open, as there could be a wide variety of situations. (P6)\\n\\n\\n\\nOne of the key benefits of having unrestricted sensor values was the possibility of features being executed in ways even the participants who implemented the feature had not anticipated. In commonly used IoT-based smart homes, a feature could not operate beyond the situations that they directly set, making this an entirely new experience that VLM camera sensors could offer.\\n\\nIn smart homes, I\\u2019m usually less impressed since I create all the features. With IoT, it\\u2019s more like, \\u2018oh, it works as expected.\\u2019 But with cameras, the system can execute features I didn\\u2019t anticipate, like identifying inefficient habits I wasn\\u2019t even aware of and offering helpful advice\\u2014that\\u2019s a big advantage. (P12)\\n\\n\\n\\nHowever, this characteristic also introduced new challenges in the process of building smart home features. Since sensor values were not limited to a few predefined lists, it became difficult to follow the traditional trigger-action paradigm. Instead, participants had to set the sensor values as variables and configure actions to execute automatically based on those values. For example, P3 requested that the system automatically select and play suitable music based on her detected emotional state. Also, as the range of possible actions increased, participants grew concerned about features they had not anticipated. To address these concerns, some participants wanted the smart home system to propose potential scenarios where the features could be applied.\\n\\n\\nIn conclusion, VLM camera sensors opened up the possibility of executing features that participants had not anticipated. However, this required building features based on variables, and participants needed additional support to help them account for the wide range of possible scenarios where features could be triggered.\\n\\n\\n\\n\\n4.2.5. Interpretation Beyond Sensing: Building Features Without Explicit Definitions\\n\\nParticipants expected that VLM camera sensors could go beyond merely recognizing current situations and perform additional interpretative tasks. They built smart home features using the VLM camera sensor\\u2019s ability to (1) provide useful information, (2) make judgments about the current state, (3) draw comprehensive conclusions, and (4) offer recommendations. As described in Section 4.1, this capability contributed to building diverse smart home features.\\n\\n\\nParticipants perceived the VLM camera sensor as more than just a tool for sensing their home\\u2014it was capable of \\u201dthinking\\u201d based on its observations. By moving beyond simple sensing to interpreting situations, these sensors enabled participants to build complex features without explicit definitions, streamlining processes and allowing for capabilities that were previously impossible.\\n\\nIf I were to build a feature with an IoT sensor to notify me when my desk is dirty, I\\u2019d be stuck trying to define what \\u2018dirty\\u2019 means. But with the camera sensor, it can look at the state of my desk and judge whether it\\u2019s dirty or not. This made building the feature much simpler. (P10)\\n\\n\\n\\nHowever, the sensor\\u2019s ability to interpret situations independently sometimes resulted in differences between human and camera sensor interpretations. As a result, participants had to refine and align these differences. To address this, they employed strategies that involved sharing personal information with the sensor, helping it better understand them.\\n\\nWhile the posture warning feature alerts me to things like resting my chin on my hand or shaking my leg, those don\\u2019t seem critical to me. However, a severely slouched posture is important because I have back problems, so I made sure to inform the system about my bad back. (P4)\\n\\n\\nMy air conditioner gives off a musty smell, so we have to briefly open and close the door when we first turn it on. But GPT wouldn\\u2019t understand that and would probably flag it as wasted energy, so I had to let it know to exclude that situation. (P1)\\n\\n\\n\\nIn conclusion, while VLM camera sensors contributed to building a variety of smart home features by interpreting context beyond simple recognition, participants needed support to align sensor interpretations with human expectations during development.\\n\\n\\n\\n\\n\\n4.3. What Are the Concerns Regarding VLM Camera Sensor-Based DIY Smart Homes?\\n\\nThrough this study, we were able to gather users\\u2019 concerns as well. Since using a camera sensor requires installing cameras inside the home, participants expressed concerns related to privacy issues (Section 4.3.1). Also, the video footage could not only be recorded but also analyzed, raising new concerns that surpassed the issues traditionally associated with basic home cameras (Sections 4.3.1\\u20134.3.4). In interviews, 5 out of 12 participants (P1, P6, P7, P10, P12) explained that beyond simply recording footage, the analysis of this footage led to additional concerns. This section will discuss four key concerns raised by participants.\\n\\n\\n\\n4.3.1. Concerns about Privacy Issues from Full Digitalization of Home Life\\n\\nOver the three-week period, participants raised various privacy concerns, many of which aligned with those identified in previous studies on home cameras (Jin et\\u00a0al., 2022; Chalhoub et\\u00a0al., 2021; Jin et\\u00a0al., 2022). Specifically, participants emphasized the need for design solutions that go beyond ensuring technical security, highlighting the importance of psychological comfort. Accordingly, they stressed the need for features that clearly indicate when the camera is operating and provide easy control over its functions.\\n\\n\\nMoreover, VLM camera sensors introduced an additional layer of concern, as they not only capture video but also analyze it. Participants were particularly worried that the analysis and digitization of images could expose sensitive information, raising privacy issues. They feared that unintended details might be detected, analyzed, and used as materials for smart home features, potentially leading to the overexposure of their private lives.\\n\\nWhen I take photos on my phone, they automatically update on my Nest Hub. Seeing that those photos are analyzed made me think, \\u2018this is a bit of an invasion of privacy.\\u2019 If the system can reference those photos, it could figure out where I\\u2019ve been or what I did outdoors, which felt a bit uncomfortable. (P1)\\n\\n\\nIf I\\u2019m having a private conversation with a friend, and I find out that the messages are being continuously analyzed, it wouldn\\u2019t feel great. (P10)\\n\\n\\n\\nParticipants also raised concerns about the potential commercial use of information collected inside their homes. They feared that their personal data could be misused to generate indiscriminate product recommendations benefiting corporations, which heightened their unease about privacy and data exploitation.\\n\\nIt could recommend clothes or detergent I need, but if I think about it negatively, they could be using my data for advertising, which would end up disrupting my daily life. (P7)\\n\\n\\n\\nIn summary, participants expressed concerns about the digitalization of their daily lives through camera sensors, particularly regarding the unintended analysis of personal information and its use in smart home features, as well as the potential commercial exploitation of their data. These concerns highlighted the need for transparency and user control over how their information is processed and utilized.\\n\\n\\n\\n\\n4.3.2. Concerns About Replacing Interactions Between Family Members\\n\\nSome of the features participants built had the potential to replace existing interactions between family members. P2 explained that he considered introducing features to avoid nagging their child, while P1 wanted to help their parents take their supplements correctly.\\n\\nMy daughter often reads or draws without turning the light on. When I noticed later, I ended up nagging her, so I thought it would be helpful if the light turned on automatically in real time. (P2)\\n\\n\\nMy parents are taking all the supplements they have without much thought. So, I thought it would be helpful if the smart home could track what they\\u2019ve eaten today and provide information on which supplements they still need to take. (P1)\\n\\n\\n\\nHowever, participants also expressed concern that such a feature could reduce social interactions within the family.\\n\\nWhen my sister forgets her car key and leaves the house, she usually calls me, and I have to look for it, which leads to a bit of bickering. But if everything worked perfectly, even those small interactions could disappear, and that\\u2019s something I was concerned about. (P8)\\n\\n\\nFeatures like guiding my parents to take their supplements correctly or informing them if something shouldn\\u2019t go in the fridge could be useful when I\\u2019m not home. But when I\\u2019m there, I think they\\u2019d rather hear it from me than from the system. (P1)\\n\\n\\n\\nIn summary, participants were concerned that smart home features might reduce meaningful family interactions and wanted to ensure that systems were not designed to diminish these important connections.\\n\\n\\n\\n\\n4.3.3. Concerns About Over Dependence on Automated Decision-Making\\n\\nParticipants explained that even simple features quickly became indispensable once integrated into their routine. They expressed concern that, as more intelligent features are added, they might become overly reliant on them.\\n\\nMy parents can\\u2019t live without the counter sensor now. It\\u2019s just a simple feature that automatically turns on the bathroom light, but once we get used to it, we can\\u2019t live without it. I think as more smart feature becomes a part of daily life, we\\u2019ll end up depending on it even more. (P1)\\n\\n\\n\\nSome participants also worried that smart home features could interfere with their positive habits as smart homes become capable of making automated decisions.\\n\\nWe all have our own cleaning routines, but if we become too dependent on the smart home, I might only clean the areas the smart home tells me are dirty. In a way, this could end up disrupting the good habits I\\u2019ve already developed. (P1)\\n\\n\\n\\nTo address these concerns, some participants suggested that a good solution would be to set the smart home features to activate only when they specifically wanted them.\\n\\nI like to think things through as much as possible and try to figure out the answer myself before seeing what others think. So, I\\u2019d prefer if the system only gave me answers when I physically pressed a button. (P8)\\n\\n\\n\\nIn conclusion, participants wanted to carefully consider when and how these features should operate, ensuring they avoid over-reliance on technology that could negatively affect their well-being.\\n\\n\\n\\n\\n4.3.4. Concerns and Discomfort About AI Controlling Daily Life\\n\\nAll participants mentioned being surprised by the detailed descriptions the AI provided when analyzing images. Some expressed concerns about AI becoming too intrusive in their daily lives.\\n\\nWe sometimes humanize AI, and the thought of AI watching and analyzing my home\\u2014whether on-device or not\\u2014feels no different. It\\u2019s like a stranger observing my daily life. (P6)\\n\\n\\n\\nParticipants particularly felt discomfort when they perceived AI as trying to analyze and excessively improve or control their behavior. This led to stronger feelings of rejection toward the technology.\\n\\nAI judging my actions can make me feel like I\\u2019m being watched, which creates a sense of discomfort. That\\u2019s one of the things I was concerned about. (P12)\\n\\n\\nWhen AI analyzed my behavior and suggested that I seemed anxious based on how I was holding a can, it felt like it was constantly trying to judge and correct me, which made me really uncomfortable. (P3)\\n\\n\\n\\nParticipants were particularly wary of how AI control could steer their daily lives in unintended directions or negatively influence their decisions.\\n\\nIf it provides information tailored to my situation, I feel like I have to follow that advice, even though I wouldn\\u2019t know if it\\u2019s correct. If it were to suggest actions that are wrong or harmful to me, it could become a seriously threatening experience for my life. (P10)\\n\\n\\n\\nIn summary, participants expressed concerns about AI becoming too intrusive in their lives, especially when it appeared to judge or correct them. This sense of being monitored and controlled by AI caused discomfort and unease.\\n\\n\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAumxzrFwvvf",
        "outputId": "ba772142-be49-48ea-9111-9c7557ba7192"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1695, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Embeddings"
      ],
      "metadata": {
        "id": "RzHRBKmPxy1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6R8eNSGx6Qj",
        "outputId": "258863d0-42ff-413a-cf9f-35b33917a15d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "H1gaRL1_xEg5"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnJnYLhyy3ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_ai_key=userdata.get('open_ai_key')\n",
        "os.environ['OPENAI_API_KEY'] = open_ai_key\n",
        "client = OpenAI()\n",
        "# client.api_key =\n",
        "def get_embedding(text, model='text-embedding-ada-002'):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
      ],
      "metadata": {
        "id": "L_RDjNMSySal"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(get_embedding(paragraphs[\"paragraph\"].iloc[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r069XIwPxrOu",
        "outputId": "934e7789-ab24-4d53-e153-d4c72c430221"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_XxMImcP0Gdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "paragraphs[\"embeddings\"] = paragraphs[\"paragraph\"].progress_apply(get_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF4MWpMpzUcg",
        "outputId": "6e95b4a9-341a-4f88-8459-02d1e9ba215d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 98/1695 [00:34<13:41,  1.94it/s]"
          ]
        }
      ]
    }
  ]
}